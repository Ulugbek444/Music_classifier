

{'eval_loss': 1.0366945266723633,
 'eval_runtime': 51.6656,
 'eval_samples_per_second': 269.444,
 'eval_steps_per_second': 33.697,
 'epoch': 1.0}

{'eval_loss': 1.0366945266723633,
 'eval_macro_f1': 0.6347144061389928,
 'eval_weighted_f1': 0.6341243988006784,
 'eval_runtime': 54.684,
 'eval_samples_per_second': 254.572,
 'eval_steps_per_second': 31.837,
 'epoch': 1.0}

                   precision    recall  f1-score   support

       anger       0.53      0.64      0.58      2371
        fear       0.67      0.62      0.65      2080
         joy       0.67      0.58      0.62      3780
        love       0.66      0.73      0.69      2043
     sadness       0.65      0.65      0.65      3150
    surprise       0.65      0.59      0.62       497

    accuracy                           0.63     13921
   macro avg       0.64      0.64      0.63     13921
weighted avg       0.64      0.63      0.63     13921


tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–Ω—É—Ç—Ä–∏:
—á–∏—Ç–∞–µ—Ç—Å—è tokenizer.json / vocab.txt
–ø–æ–¥–≥—Ä—É–∂–∞–µ—Ç—Å—è:
WordPiece —Å–ª–æ–≤–∞—Ä—å
special tokens ([CLS], [SEP])
üìå Tokenizer –ù–ï —É—á–∏—Ç—Å—è, –æ–Ω —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω.

model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
–ü—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç:
—á–∏—Ç–∞–µ—Ç—Å—è config.json
–∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏
–ø–æ–¥–Ω–∏–º–∞–µ—Ç—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
BERT encoder
Classification head (Linear)
üìå –¢—ã –ù–ï –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë—à—å –º–æ–¥–µ–ª—å –≤—Ä—É—á–Ω—É—é ‚Äî —ç—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ.

model.eval()
‚ö†Ô∏è –û–ß–ï–ù–¨ –í–ê–ñ–ù–û
–≠—Ç–æ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç –º–æ–¥–µ–ª—å –≤ inference mode:
‚ùå dropout –≤—ã–∫–ª—é—á–µ–Ω
‚ùå batchnorm —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω
‚úÖ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ç–∞–±–∏–ª—å–Ω—ã
‚úÖ –±—ã—Å—Ç—Ä–µ–µ
üìå –ë–µ–∑ —ç—Ç–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –ù–ï –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω.

inputs = tokenizer(
    text,
    return_tensors="pt",
    truncation=True,
    max_length=128
)

üîπ return_tensors="pt"
üëâ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç torch.Tensor
{
  "input_ids": tensor([[101, 2023, ...]]),
  "attention_mask": tensor([[1, 1, ...]])
}
‚úÖ –ò–º–µ–Ω–Ω–æ —Ç–æ, —á—Ç–æ –∂–¥—ë—Ç PyTorch –º–æ–¥–µ–ª—å.

üîπ truncation=True
–ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –¥–ª–∏–Ω–Ω–µ–µ max_length:
–æ–±—Ä–µ–∑–∞–µ—Ç—Å—è
–±–µ–∑ –æ—à–∏–±–æ–∫
üìå –≠—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è lyrics ‚Äî –æ–Ω–∏ –¥–ª–∏–Ω–Ω—ã–µ.

üîπ max_length=128
–ü–æ—á–µ–º—É 128:
–º–æ–¥–µ–ª—å –æ–±—É—á–∞–ª–∞—Å—å —Å 128 ‚úÖ
–±—ã—Å—Ç—Ä–µ–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å ‚úÖ
–º–µ–Ω—å—à–µ VRAM ‚úÖ
—Ç–µ–∫—Å—Ç—ã –ø–µ—Å–µ–Ω –æ—á–µ–Ω—å –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è ‚úÖ
üî• –ê–±—Å–æ–ª—é—Ç–Ω–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –≤—ã–±–æ—Ä.

üîπ with torch.no_grad():
üëâ –ì–æ–≤–æ—Ä–∏—Ç PyTorch:
‚ùå –Ω–µ —Å—á–∏—Ç–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
‚ùå –Ω–µ —Ö—Ä–∞–Ω–∏—Ç—å computation graph
‚úÖ —ç–∫–æ–Ω–æ–º–∏—Ç—å –ø–∞–º—è—Ç—å
‚úÖ —É—Å–∫–æ—Ä—è—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
üìå –≠—Ç–æ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è inference.

üîπ outputs = model(**inputs)
–ú–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç —Ç–∏–ø–∞:
SequenceClassifierOutput(
  logits=tensor([[...]]),
  hidden_states=None,
  attentions=None
)
üìå –ù–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç —Ç–æ–ª—å–∫–æ logits.

üîπ probs = torch.softmax(outputs.logits, dim=1)
–ü–æ—á–µ–º—É softmax:
logits ‚Üí –Ω–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
softmax ‚Üí –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ –∫–ª–∞—Å—Å–∞–º
—Å—É–º–º–∞ = 1 ‚úÖ
üìå dim=1, –ø–æ—Ç–æ–º—É —á—Ç–æ:
[batch_size, num_classes]

üîπ pred_id = probs.argmax(dim=1).item()
üîπ confidence = probs.max().item()
argmax ‚Üí –∏–Ω–¥–µ–∫—Å –∫–ª–∞—Å—Å–∞
max ‚Üí —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
üìå –î–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:
pred_id = int
confidence = float

üîπ label = model.config.id2label[pred_id]
üìå –≠—Ç–æ:
–º–∞–ø–ø–∏–Ω–≥ –∏–∑ –æ–±—É—á–µ–Ω–∏—è
—Å–∞–º–æ–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –º–µ—Å—Ç–æ –±—Ä–∞—Ç—å labels
‚ùå –ù–ï –∑–∞—Ö–∞—Ä–¥–∫–æ–∂–µ–Ω—ã
‚úÖ –ø–µ—Ä–µ–Ω–æ—Å–∏–º—ã
‚úÖ –±–µ–∑–æ–ø–∞—Å–Ω—ã

üîπ return label, confidence
("sadness", 0.87)